import firebase_admin
from firebase_admin import credentials, firestore
from firebase_admin import credentials, db
from PySide6.QtQml import QQmlApplicationEngine
from PySide6.QtWidgets import QApplication
from pathlib import Path
import sys
from datetime import datetime
import threading
from difflib import SequenceMatcher
from pywinauto import application
import math
from pynput import mouse, keyboard
import re
import os
import time
import numpy as np
import pyautogui
import pytesseract
import cv2
from PIL import ImageGrab
from OpenGL.GLU import *
from OpenGL.GL import *
import psutil
from PySide6.QtGui import QSurfaceFormat
from PySide6.QtCore import QObject, Slot
from geopy.geocoders import Nominatim
# Android
from plyer import gps
from kivy.clock import Clock
from kivy.utils import platform
def on_location(**kwargs):
    print(
        kwargs['lat'],
        kwargs['lon'],
        kwargs.get('altitude'),
        kwargs.get('speed')
    )

gps.configure(on_location=on_location)
gps.start(minTime=1000, minDistance=0)

if platform == 'android':
    from plyer import accelerometer
    accelerometer.enable()

def read_imu(dt):
    val = accelerometer.acceleration
    if val:
        ax, ay, az = val
        print(ax, ay, az)

Clock.schedule_interval(read_imu, 1/50)

# --- åŸºç¤è¨­å®š --- python "D:\Python\Non-codeAutomaticOperation\UIA.py"
pytesseract.pytesseract.tesseract_cmd = r"C:\Users\USER\AppData\Local\Programs\Tesseract-OCR\tesseract.exe"
base_path = getattr(sys, '_MEIPASS', os.path.dirname(
    os.path.abspath(__file__)))
TEMPLATE_DIR = os.path.join(base_path, 'templates')
MATCH_THRESHOLD = 0.85
LANGS = "eng+chi_sim"
DEBUG = True

# --- å…±ç”¨å·¥å…· ---
alive_event = threading.Event()
cred = credentials.Certificate("serviceAccountKey.json")
firebase_admin.initialize_app(cred, {
    "databaseURL": "https://ä½ çš„å°ˆæ¡ˆ-id.firebaseio.com/"
})


def resource_path(relative_path):
    # åœ¨ EXE æ™‚è®€å– sys._MEIPASSï¼Œå¦å‰‡è®€åŸè·¯å¾‘
    if hasattr(sys, '_MEIPASS'):
        return os.path.join(sys._MEIPASS, relative_path)
    return os.path.join(os.path.abspath("."), relative_path)


def watchdog():
    while not alive_event.wait(timeout=10):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        cv2.imwrite(f"debug_{ts}.png", screenshot())
        print(f"[Watchdog] ä¸»ç·šç¨‹å¯èƒ½å¡æ­»ï¼Œå·²ä¿å­˜ debug_{ts}.png")


def screenshot():
    """æˆªå–å…¨å±ä¸¦è½‰æˆOpenCVåœ–åƒ  RGB â†’ BGR """
    img = np.array(ImageGrab.grab())
    return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)


def locate_template_orb(name, sort=1, num=1, extractor=False):
    """ORB ç‰¹å¾µåŒ¹é…æ‰¾åœ–åƒ screenshot() â†’ ç°éš """
    name = name.split("<img>")[1]
    path = os.path.join(TEMPLATE_DIR, f"{name}.png")
    if not os.path.exists(path):
        return None
    tpl = cv2.imread(path, 0)
    screen_gray = cv2.cvtColor(screenshot(), cv2.COLOR_BGR2GRAY)
    # å¯èª¿æ•´ç‰¹å¾µæ•¸ï¼Œè¶Šå°‘è¶Šå¿«ï¼Œ100å°åœ–æ¨™æˆ–æŒ‰éˆ•ã€300ä¸€èˆ¬GUIå…ƒç´ ã€500è¤‡é›œç•«é¢(ä¾‹å¦‚ Hierarchy)
    orb = cv2.ORB_create(400)
    kp1, des1 = orb.detectAndCompute(tpl, None)
    kp2, des2 = orb.detectAndCompute(screen_gray, None)
    if des1 is None or des2 is None:
        return None
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = sorted(bf.match(des1, des2), key=lambda m: m.distance)
    if len(matches) < 5:
        return None  # å¤ªå°‘ç‰¹å¾µé…å°è¦–ç‚ºä¸å¯é 
    # å–å‰ 10 å€‹æœ€ä½³åŒ¹é…é»çš„åº§æ¨™
    pts = [(int(kp2[m.trainIdx].pt[0]), int(kp2[m.trainIdx].pt[1]))
           for m in matches[:10]]
    pts.sort(key=lambda p: (p[0], p[1]))  # å·¦ä¸Šæ’åº
    if not pts:
        TargetExtractor().select_polygon_roi()
        
    # é¸å–é»
    if sort == "å¥‡æ•¸":
        pts = pts[::2]
    elif sort == "å¶æ•¸":
        pts = pts[1::2]
    elif isinstance(sort, int):
        idx = sort - 1 if sort > 0 else sort
        return [pts[idx]] if -len(pts) <= idx < len(pts) else []
    # è™•ç† numï¼ˆæ­£æ•¸å–å‰ numï¼Œè² æ•¸å–å€’æ•¸ abs(num)ï¼‰
    if num != 1:
        pts = pts[:num] if num > 0 else pts[num:]
    return pts


def locate_template_orb_cached(obj, name, sort=1, num=1):
    if name in obj.cache:
        pos = obj.cache[name]
        if validate_cache(name, pos):
            return pos
    pos = locate_template_orb(name, sort, num, extractor=obj.extractor)
    if pos:
        obj.cache[name] = pos
    return pos


def validate_cache(name, pos, tolerance=10):
    screen_gray = cv2.cvtColor(screenshot(), cv2.COLOR_BGR2GRAY)
    h, w = screen_gray.shape[:2]
    x, y = pos
    # å®‰å…¨é‚Šç•Œ
    x1, y1 = max(x - tolerance, 0), max(y - tolerance, 0)
    x2, y2 = min(x + tolerance, w), min(y + tolerance, h)
    region = screen_gray[y1:y2, x1:x2]
    tpl = cv2.imread(os.path.join(TEMPLATE_DIR, f"{name}.png"), 0)
    if tpl is None or region.size == 0:
        return False
    res = cv2.matchTemplate(region, tpl, cv2.TM_CCOEFF_NORMED)
    _, max_val, _, _ = cv2.minMaxLoc(res)
    return max_val > 0.8


def locate_text(keyword, sort=1, num=1, classA=None):
    """æ‰¾å­—"""
    # OCRè­˜åˆ¥æ–‡å­—
    data = pytesseract.image_to_data(cv2.cvtColor(screenshot(
    ), cv2.COLOR_BGR2GRAY), lang=LANGS, output_type=pytesseract.Output.DICT)
    # æ”¶é›†åŒ¹é…é»
    pts = [
        (data['left'][i] + data['width'][i] // 2,
         data['top'][i] + data['height'][i] // 2)
        for i, t in enumerate(data['text'])
        # SequenceMatcher ç›¸ç¬¦æ¯”ä¾‹
        if t.strip() and SequenceMatcher(None, t.lower(), keyword.lower()).ratio() >= 0.7
    ]
    if not pts:
        if DEBUG:
            print(f"âš ï¸ æ‰¾ä¸åˆ°åŒ¹é…é»ï¼š{keyword}ã€‚è‹¥ç›®æ¨™åœ¨å ´å‰‡å»ºè­°")
        return None
    # æ’åºï¼ˆå·¦ä¸Šå„ªå…ˆï¼‰
    pts.sort(key=lambda p: (p[0], p[1]))
    # sort æ•´æ•¸ â†’ æŒ‡å®šä½ç½®ï¼›å¥‡å¶ â†’ ç¯©é¸ï¼›å¦å‰‡å›å‚³å‰ num å€‹ # åºåˆ—[start:end:step]
    if sort == "å¥‡æ•¸":
        pts = pts[::2]
    elif sort == "å¶æ•¸":
        pts = pts[1::2]
    elif isinstance(sort, int):
        idx = sort - 1 if sort > 0 else sort
        return pts[idx] if -len(pts) <= idx < len(pts) else None
    # è™•ç† numï¼ˆæ­£æ•¸å–å‰ numï¼Œè² æ•¸å–å€’æ•¸ abs(num)ï¼‰
    if num != 1:
        pts = pts[:num] if num > 0 else pts[num:]
    if classA is None:
        return pts
    else:
        # * æ‰¾classA çš„å…§å®¹ï¼Œé è¨­æ˜¯ # æ‰¾ classA çš„é€™ä¸€è¡Œ classAå¾Œé¢
        readText = [
            t
            for t in data['text']
            if t.strip() and SequenceMatcher(None, t.lower(), keyword.lower()).ratio() >= 0.7
        ]

        # *** classA ä¼¼ä¹åœ¨é€™ä¸€è¡Œé–‹å§‹ä¸é€šç”¨äº†ï¼Œä½¿ç”¨åˆ° Geocoding
        # *** firebase ç”¨æˆ¶å„²å­˜çš„èµ·é»åœ°å€ addrStart
        geolocator = Nominatim(user_agent="geo_example")
        startP=firestore.client("ç”¨æˆ¶").reference("addrStart").get()
        nearP = firestore.client("ç”¨æˆ¶").document("near").get().to_dict()
        farP = firestore.client("ç”¨æˆ¶").document("far").get().to_dict()
        locationStart = geolocator.geocode(startP)
        locationNear = geolocator.geocode(startP)
        locationFar = geolocator.geocode(startP)
        # é–“è·å¤ªè¿‘(firestore.client().reference(å¤ªè¿‘çš„åœ°å€)ï¼Œèµ·é»å’Œå¤ªè¿‘åœ°å€çš„è·é›¢ç‚º é–“è·)çš„ä¸€äº›åœ°å€ç‚ºä¸€åˆ†æ”¯ manifest[åˆ†æ”¯]ï¼Œé›¢èµ·é»å¤ªé (firestore å¤ªé åœ°å€)é¡å¤–å®‰æ’ manifest2
        NEAR_DISTANCE =dist(nearP,startP)
        FAR_DISTANCE =dist(farP,startP)
        def dist(a,b):
            aLocation=geolocator.geocode(a) 
            # é¿å…è¢«geocode å°é–
            time.sleep(0.5)
            if b==startP:
                bLocation=locationStart
            elif b==nearP:
                bLocation=locationNear
            elif b==farP:
                bLocation=locationFar
            else:
                bLocation=geolocator.geocode(b)
            if aLocation or bLocation is None:
                print("ç„¡æ•ˆåœ°å€")
            distance =(aLocation.latitude - bLocation.latitude)**2 + (aLocation.longitude - bLocation.longitude)**2
            time.sleep(0.5)
            return distance
        
        for ress in readText:
            line_key = (
                data['block_num'][ress],
                data['par_num'][ress],
                data['line_num'][ress]
            )
            addresses = []
            for j, t in enumerate(data['text']):
                if not t.strip():
                    continue
                if j < ress:
                    continue
                if (data['block_num'][j], data['par_num'][j], data['line_num'][j]) != line_key:
                    continue

                addresses.append({
                    "address": t,
                    "distance": dist(t,startP),
                    # ***ä½¿ç”¨ æ‰¾åœ°å€æ™‚ï¼Œé †ä¾¿ æ‰¾è²¨å“
                    # *** æœå°‹ç›¸ç¬¦æ–‡å­—çš„è²¨å“ä¹˜ä¸Šæ•¸é‡ï¼Œä¸¦è¨ˆç®—ç–ŠåŠ çš„ç©ºé–“å¤§å°ï¼Œä»¥ç–ŠåŠ å¤§å°ä¾†æ’åº
                    "goods":""
                })
            addresses.sort(key=lambda x: x["distance"])
            # å»ºç«‹ manifest åˆ†æ”¯ï¼ˆè¿‘ / é ï¼‰ # ç”¨æˆ¶èªªåˆ†æ”¯ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯èªªå…¶ä»–æ±è¥¿
            manifest_near = [
                {"address": addresses[i]["address"], "goods": addresses[i]["goods"]}
                for i in range(len(addresses)-1)  # ç”¨ index æ‰èƒ½æ‹¿ä¸‹ä¸€ç­†
                if addresses[i]["distance"] <= NEAR_DISTANCE
                and abs(addresses[i]["distance"] - addresses[i+1]["distance"]) <= NEAR_DISTANCE
            ]
            manifest_far = [
                {"address": info["address"], "goods": info["goods"]}
                for info in addresses
                if info["distance"] >= FAR_DISTANCE
            ]
            manifest = [manifest_near, manifest_far]
            # *** goods æ’åˆ—åœ¨æœ‰é™ç©ºé–“ï¼Œè¨ˆç®—manifesté›£åº¦ æ’åº
            # 4ï¸âƒ£ ä¸Šå‚³ Firebase
            # manifest ä¸Šå‚³çµ¦firebaseï¼Œmanifestä¸­æœ€é›£çš„çµ¦æœ€æ—©è«‹æ±‚çš„ç”¨æˆ¶ # *** firebase åˆ†ç™¼çµ¦ç”¨æˆ¶ï¼Œç”¨æˆ¶å¦‚ä½•ç²å– manifest
            
            firestore.client().document("manifest").add(manifest)

            
                # *** ç¹ªè£½è·¯ç·šåœ–ä¸¦è¨˜éŒ„æŒ‡å—é‡æ–¹å‘ï¼Œæ—‹è½‰åœ°åœ–æ™‚è·¯ç·šåœ–èˆ‡åœ°åœ–çš„æŒ‡å—é‡å‘é‡ çŸ¯æ­£
                # *** æŒ‡å—é‡è¨ˆç®—(ä¸€ç¶­)
                # Routing APIçµ¦æœ€ä½³çœŸå¯¦è·¯ç·š

from geographiclib.geodesic import Geodesic

def click(pos): pyautogui.moveTo(
    *pos, duration=0.2); pyautogui.click(); time.sleep(0.3)


class InputCommand(QObject):
    def __init__(self):
        super().__init__()
        self.vars = {}
        self.current_window = None
        self.cache = {}
        self.extractor = True
        self.app = None

    def focus_window(self, title):
        title_pattern = fr'^{title}.*'
        try:
            app = application(backend="uia").connect(title_re=title_pattern)
            app.window(title_re=title_pattern).set_focus()
            self.current_window = title
            print(f"ğŸ§  èšç„¦ [{title}]")
        except Exception as e:
            print(f"âŒ ç„¡æ³•èšç„¦ [{title}]: {e}")

    def selected(self, str, sort=1, num=1, classA=None):
        if "<img>" in str:
            return locate_template_orb_cached(str, sort, num)
        else:
            return locate_text(str, sort, num, classA)

    @Slot(str)
    def input_line(self, user_input):
        m = re.match(r"<\s*(.+)\s*>", user_input)
        if m:
            cmd_type = m.group(1).strip()
            match cmd_type:
                case "éŒ„è£½":
                    raw = input("è«‹è¼¸å…¥è¦éŒ„è£½çš„å‘½ä»¤(å¤šè¡Œç”¨::åˆ†éš”): ").strip()
                    rec.record(raw)
                case "æ’­æ”¾":
                    var = input("æ’­æ”¾å“ªå€‹éŒ„è£½è®Šæ•¸: ").strip() or None
                    rec.play(ic, var)
                case "æª¢è¦–éŒ„è£½":
                    rec.view()
                case "é‡æ–°å‘½å":
                    old_name = input("åŸè®Šæ•¸å: ").strip()
                    new_name = input("æ–°è®Šæ•¸å: ").strip()
                    rec.rename(old_name, new_name)
                case "å–æ¶ˆè‡ªå‹•ç¢ºèªç›®æ¨™":
                    ic.extractor = False
                    print("âœ… å·²é—œé–‰è‡ªå‹•ç¢ºèªç›®æ¨™æ¨¡å¼")
                case "ç§»é™¤":
                    var = input("ç§»é™¤å“ªå€‹éŒ„è£½è®Šæ•¸: ").strip() or None
                    rec.remove(ic, var)
                case "æ•´ç†è·¯ç·š":  # ***æ•´ç†è·¯ç·š
                    var = input("å·²é–‹å•Ÿ æ•´ç†è·¯ç·š ").strip() or None
                    self.selected("åœ°å€", 1, 1, "åœ°å€")

                case "è·é›¢å¤šå°‘":  # ***å’Œä¸‹ä¸€å€‹åœ°å€ è·é›¢å¤šå°‘
                    var = input("å·²ç¹ªè£½åœ°åœ– ").strip() or None
                    def real_dist(p, q):
                        return Geodesic.WGS84.Inverse(
                            p.lat, p.lon, q.lat, q.lon
                        )['s12']
                case "ç¹ªåœ–":  # ***ç¹ªåœ–
                    var = input("å·²ç¹ªè£½åœ°åœ– ").strip() or None

                case _:
                    print(f"âš ï¸ æœªçŸ¥æŒ‡ä»¤: {cmd_type}")
        else:
            # æ™®é€šå‘½ä»¤ç›´æ¥åŸ·è¡Œ
            cmds = user_input.split("::")
            ic.execute_line(cmds)

    def execute_line(self, lines):
        for line in lines:
            try:
                window, path, action = [x.strip() for x in line.split(',', 2)]
                if self.current_window != window:
                    self.focus_window(window)
                    time.sleep(0.5)
                # *éµç›¤æ»‘é¼ 
                # -*- coding: utf-8 -*- # æ»‘é¼  + éµç›¤å…¨åŠŸèƒ½ç¤ºä¾‹ï¼ˆä¸å«ç›£è½ï¼‰ import pyautogui, keyboard, time from pynput.mouse import Button, Controller as MouseController from pynput.keyboard import Key, Controller as KeyController # pyautogui å…¨åŸŸè¨­å®š pyautogui.FAILSAFE = True pyautogui.PAUSE = 0.1 # ==== æ»‘é¼ æ§åˆ¶ ==== # ä½ç½®è³‡è¨Š screen_w, screen_h = pyautogui.size() print("Screen:", screen_w, screen_h) print("Mouse position:", pyautogui.position()) # åŸºæœ¬ç§»å‹• pyautogui.moveTo(100, 100, duration=0.3) pyautogui.moveRel(50, 0, duration=0.2) # é»æ“Šèˆ‡é›™æ“Š pyautogui.click() pyautogui.doubleClick() pyautogui.rightClick() pyautogui.middleClick() pyautogui.click(300, 300) # æŒ‰ä¸‹ / æ”¾é–‹ï¼ˆå¯é•·æŒ‰ï¼‰ pyautogui.mouseDown(button='left') time.sleep(0.5) pyautogui.mouseUp(button='left') # æ‹–æ›³æ“ä½œ pyautogui.moveTo(400, 400) pyautogui.mouseDown() pyautogui.moveTo(600, 600, duration=1.0) pyautogui.mouseUp() # æ»¾è¼ª pyautogui.scroll(300) pyautogui.scroll(-300) # ==== éµç›¤æ§åˆ¶ ==== # è¼¸å…¥æ–‡å­— pyautogui.typewrite("Hello from pyautogui!", interval=0.05) # å–®éµæ“ä½œ pyautogui.press("enter") pyautogui.press("tab") pyautogui.press("backspace") # çµ„åˆéµ pyautogui.hotkey("ctrl", "s") pyautogui.hotkey("alt", "f4") # æ‹†è§£æŒ‰ä¸‹èˆ‡æ”¾é–‹ pyautogui.keyDown("shift") pyautogui.press("a") pyautogui.keyUp("shift") # ==== ä½¿ç”¨ pynput é€²éšæ§åˆ¶ ==== mouse = MouseController() keyboard_ctrl = KeyController() # æ»‘é¼ ç²¾ç¢ºæ§åˆ¶ mouse.position = (200, 200) mouse.press(Button.left) time.sleep(0.3) mouse.release(Button.left) mouse.press(Button.right) mouse.release(Button.right) mouse.scroll(0, 3) # éµç›¤ç²¾ç¢ºæ§åˆ¶ keyboard_ctrl.press('a') keyboard_ctrl.release('a') keyboard_ctrl.press(Key.enter) keyboard_ctrl.release(Key.enter) # ==== ä½¿ç”¨ keyboard æ¨¡çµ„ ==== keyboard.press_and_release('ctrl+c') keyboard.write('Typed by keyboard module!', delay=0.05) if keyboard.is_pressed('esc'): print("ESC pressed!")
                # *é‹ç®—
                # -*- coding: utf-8 -*- import math, random, statistics, decimal, fractions, cmath, numpy as np # === åŸºæœ¬å››å‰‡ === a, b = 10, 3 print(a + b, a - b, a * b, a / b, a // b, a % b, a ** b) # === æ¯”è¼ƒèˆ‡é‚è¼¯ === print(a > b, a < b, a == b, a != b, a >= b, a <= b) # === å…§å»ºå‡½å¼ === print(abs(-5), round(3.14159, 2), pow(2, 5), divmod(17, 3), sum([1,2,3,4])) # === math æ¨¡çµ„ === print(math.sqrt(16), math.pow(2, 10), math.factorial(5)) print(math.sin(math.pi/2), math.cos(0), math.tan(math.pi/4)) print(math.degrees(math.pi), math.radians(180)) print(math.log(100, 10), math.log2(8), math.exp(1)) print(math.ceil(2.1), math.floor(2.9), math.trunc(-3.8)) print(math.gcd(24, 36), math.isclose(0.1+0.2, 0.3)) # === çµ±è¨ˆ === data = [2, 3, 5, 7, 11] print(statistics.mean(data), statistics.median(data), statistics.pstdev(data)) # === éš¨æ©Ÿ === print(random.random(), random.randint(1,10), random.uniform(1.5,5.5)) print(random.choice(['A','B','C'])) items = [1,2,3,4]; random.shuffle(items); print(items) # === decimal é«˜ç²¾åº¦é‹ç®— === decimal.getcontext().prec = 10 x = decimal.Decimal('1.1') + decimal.Decimal('2.2') print(x)  # ç²¾ç¢ºåŠ æ³• # === fractions åˆ†æ•¸ === f1 = fractions.Fraction(1,3); f2 = fractions.Fraction(1,6) print(f1 + f2, f1 * f2) # === è¤‡æ•¸ === z1, z2 = 2+3j, 1-1j print(z1 + z2, z1 * z2, abs(z1), cmath.phase(z1)) # === numpy é«˜éšé‹ç®— === arr = np.array([1,2,3,4,5]) print(arr + 2, arr * 3, np.mean(arr), np.std(arr)) print(np.sin(arr), np.dot([1,2,3],[4,5,6]))
                for pa in path.split(":"):
                    # [(x,y),(x,y),(x,y),...]ï¼Œsp[0]=x,yï¼Œsp[0][1]=yï¼Œæ‰“æ­»GPT
                    sp = self.selected(pa)
                    if sp is not None:
                        if pa != path.split(":")[-1]:
                            click(sp[0])
                        elif pa == path.split(":")[-1]:
                            for act in action.split(":"):
                                i = 0
                                while i < len(action):
                                    act = action[i]
                                    match act:
                                        # Unity
                                        case "é»æ“Š": click(sp[0])
                                        case "é›™æ“Š": pyautogui.doubleClick(sp[0])
                                        case "å³éµ": pyautogui.rightClick(sp[0])
                                        case "ä¸­éµ": pyautogui.middleClick(sp[0])
                                        case "æŒ‰ä¸‹": pyautogui.mouseDown(sp[0])
                                        case "æ”¾é–‹": pyautogui.mouseUp(sp[0])
                                        case "å„²å­˜": pyautogui.hotkey("ctrl", "s")
                                        case "è¤‡è£½": pyautogui.hotkey("ctrl", "c")
                                        case "è²¼ä¸Š": pyautogui.hotkey("ctrl", "v")
                                        case "å…¨é¸": pyautogui.hotkey("ctrl", "a")
                                        case "å‰ªä¸‹": pyautogui.hotkey("ctrl", "x")
                                        case "å¾©åŸ": pyautogui.hotkey("ctrl", "z")
                                        case "å–æ¶ˆå¾©åŸ": pyautogui.hotkey("ctrl", "y")
                                        case "åˆªé™¤": pyautogui.press("delete")
                                        case "èšç„¦è©²ç‰©ä»¶": pyautogui.press("f")
                                        case "é—œé–‰è¦–çª—": pyautogui.hotkey("alt", "f4")
                                        case "æ»¾ä¸Š": pyautogui.scroll(300)
                                        case "æ»¾ä¸‹": pyautogui.scroll(-300)
                                        case "å·¦æ»‘":
                                            pyautogui.dragRel(-200,
                                                              0, duration=0.5)
                                        case "å³æ»‘":
                                            pyautogui.dragRel(
                                                200, 0, duration=0.5)
                                        # è¨ˆç®—å‡ºæœ€å·¦é‚Šæœ€ä¸Šé¢çš„ä¾åºçš„ç¬¬Sä½Nå€‹ï¼Œselected(pa)ï¼Œ-3ç‚ºå€’æ•¸ç¬¬ä¸‰ä½
                                        case act if re.fullmatch(r"ç¬¬(-?\d+)ä½(\d+)å€‹", act):
                                            m = re.fullmatch(
                                                r"ç¬¬(-?\d+)ä½(\d+)å€‹", act)
                                            self.selected(
                                                pa, int(m.group(1)), int(m.group(2)))
                                        # è¨ˆç®—å‡ºæœ€å·¦é‚Šæœ€ä¸Šé¢çš„ä¾åºçš„å¶æ•¸å€‹
                                        case act if re.fullmatch(r"å¶æ•¸(\d+)å€‹", act):
                                            m = re.fullmatch(r"å¶æ•¸(\d+)å€‹", act)
                                            self.selected(
                                                pa, "å¶æ•¸", int(m.group(1)))
                                        case act if re.fullmatch(r"å¥‡æ•¸(\d+)å€‹", act):
                                            m = re.fullmatch(r"å¥‡æ•¸(\d+)å€‹", act)
                                            self.selected(
                                                pa, "å¥‡æ•¸", int(m.group(1)))
                                        case act if re.fullmatch(r"æ’åºå„²å­˜çš„(\s+)", act):
                                            m = re.fullmatch(
                                                r"æ’åºå„²å­˜çš„(\s+)", act)  # ***(å°è±¡)æ’åº
                                            a = []+m.group(1)
                                            a.sort()
                                        case act if re.fullmatch(r"è¼¸å…¥\s*(.+)", act):
                                            s = re.fullmatch(r"è¼¸å…¥\s*(.+)", act)
                                            keyboard.write(
                                                s.group(1), delay=0.05)
                                        case act if re.fullmatch(r"çµ„åˆéµ\s*(.+)", act):
                                            m = re.fullmatch(
                                                r"çµ„åˆéµ\s*(.+)", act)
                                            keys = m.group(
                                                1).split()  # ç©ºæ ¼åˆ†é–‹æ¯å€‹æŒ‰éµ
                                            pyautogui.hotkey(*keys)
                                        case act if re.fullmatch(r"ç­‰å¾…(\d+(?:\.\d+)?)ç§’", act):
                                            m = re.fullmatch(
                                                r"ç­‰å¾…(\d+(?:\.\d+)?)ç§’", act)
                                            time.sleep(float(m.group(1)))
                                        case act if re.fullmatch(r"è·é›¢\s*(.+)([<>=]+)(\d+\.?\d*)çµæŸ", act):
                                            # å³æ™‚åº§æ¨™ï¼Œè·é›¢ å°è±¡ æœ‰ å¤šé ï¼Œæœªé”æˆæ™‚ç¹¼çºŒ
                                            m = re.fullmatch(
                                                r"è·é›¢\s*(.+)([<>=]+)(\d+\.?\d*)", act)
                                            distance = math.dist(
                                                sp[0], m.group(1)[0])
                                            if eval(f"{distance:.2f}{m.group(2)}{float(m.group(3))}"):
                                                i += 2  # è·³åˆ°ã€Œä¸‹ä¸‹å€‹ã€act
                                                continue
                                            else:
                                                i += 1  # æ­£å¸¸å¾€ä¸‹
                                                continue
                                        case "é¡¯ç¤ºè©²ç›®æ¨™åº§æ¨™":
                                            print(f"ğŸ“ {pa}: {sp[0]}")
                                        case "é¡¯ç¤ºæ™‚é–“":
                                            print(
                                                f"ğŸ•’ ç¾åœ¨æ™‚é–“ï¼š{time.strftime('%H:%M:%S')}")
                                        case act if re.fullmatch(r"\s*(.+)çš„é‚è¼¯å°\s*(.+)æ€§èƒ½\s*(.+)çµæŸ", act):
                                            # è¨‚é–±äº‹ä»¶ï¼Œç›£è½m1å°m2ã€m2çš„m3 é”æˆæ™‚çµæŸä¸¦å–æ¶ˆè¨‚é–±
                                            # ç›£è½ä¸­çš„äº‹ä»¶ m1å°m2ï¼Œæœ‰ä¿®æ­£éœ€æ±‚å‰‡å›å ±ä½œæ³•ï¼Œç”šè‡³ä½¿ç”¨è€…è£œå……ä½œæ³•
                                            # ç›£è½ä¸­çš„äº‹ä»¶ m2çš„m3ï¼Œç›®å‰æ€§èƒ½ä½æ–¼ç›®æ¨™çš„70%å‰‡å›å ±ä½œæ³•ï¼Œç”šè‡³ä½¿ç”¨è€…è£œå……ä½œæ³•
                                            m = re.fullmatch(
                                                r"\s*(.+)çš„é‚è¼¯å°\s*(.+)æ€§èƒ½\s*(.+)çµæŸ", act)
                                            monitor.subscribe_event(
                                                m.group(1), m.group(2), m.group(3))
                                            # EventMonitor æŒçºŒåŸ·è¡Œ å¾ŒçºŒæŒ‡ä»¤ï¼Œé€™è¡ŒæŒ‡ä»¤å¯ä»¥è·³éäº†
                                            monitor.ic_em = re.search(
                                                fr"{m.group(2)}æ€§èƒ½{m.group(3)}çµæŸ\s*(.+)", action).group(1)
                                            print(
                                                f"å·²è¨»å†Šäº‹ä»¶ç›£è½ {m.group(1)}->{m.group(2)}->{m.group(3)}ï¼Œå¾ŒçºŒæŒ‡ä»¤äº¤ç”± EventMonitor åŸ·è¡Œ {self.ic_em}")
                                            break
                                        case act if re.fullmatch(r"ç§»é™¤\s*(.+)çš„é‚è¼¯å°\s*(.+)æ€§èƒ½\s*(.+)", act):
                                            # å–æ¶ˆç›£è½ä¸­çš„äº‹ä»¶ m1å°m2ã€m2çš„m3
                                            m = re.fullmatch(
                                                r"ç§»é™¤\s*(.+)çš„é‚è¼¯å°\s*(.+)æ€§èƒ½\s*(.+)", act)
                                            monitor.remove_subscription(
                                                m.group(1), m.group(2), m.group(3))
                                        case "æ’åº":
                                            pass
                                        case "é¡¯ç¤ºä½•ç‰©":
                                            pass
                                        case "æ’å®šä»»å‹™":

                                            pass
                                        case "è¨­å®š å³æ™‚è¨ˆç®—ç‰©é«”å¤§å°çš„ éŒ¨å®šç‰©å¤§å°":
                                            # ** æŠ“å–æ¨¡å¼ï¼Œè¨­å®šéŒ¨å®šç‰©
                                            m=re.match( r"(.*)_W(\d+)_H(\d+)_Z([\d\.]+)", act[i+1])
                                            if not m:
                                                print("è«‹ä¾ç…§åœ–ç‰‡_W0_H0_Z0æ ¼å¼")
                                                return
                                            self.selected(act[i+1])
                                            i+=2
                                            continue
                                        case "å³æ™‚è¨ˆç®—ç‰©é«”å¤§å°":
                                            # *** è¨ˆç®—æ¨¡å¼ï¼Œéœ€è¦OCRè¨ˆç®—ç‰©é«”å®¹ç©
                                            TargetExtractor().load_img_whz()
                                            pass
                                        case "ç•«é¢ç”Ÿæˆæ¨¡å‹":

                                            pass
                                        # ***è£œå……
                                    i += 1  # é è¨­æ¯æ¬¡å¾€ä¸‹ä¸€å€‹
                        # else:æ‰¾ä¸‹ä¸€å€‹è·¯å¾‘
                    else:
                        if pa == path.split(":")[-1]:
                            # æ‰¾æ»‘é¼ é™„è¿‘çš„æœå°‹æ¬„ä½åœ–ç‰‡ï¼Œè¼¸å…¥ç›®æ¨™
                            if self.selected("<img>search") is not None:
                                keyboard.write(pa, delay=0.05)
                            else:
                                print("æ²’è¾¦æ³•æ‰¾åˆ°{pa}")
                        else:
                            # æŒçºŒæ»‘å‹•æª¢æŸ¥å‰ä¸€å€‹è·¯å¾‘çš„æ•´å€‹ç•«é¢ï¼Œç›´åˆ°ç„¡è®ŠåŒ–æ™‚è·³å‡º
                            prev_img = screenshot()
                            while True:
                                if self.selected(pa) is not None:
                                    break
                                pyautogui.scroll(-300)
                                curr_img = screenshot()
                                # æ”¹ç‚ºå·®ç•°çµ±è¨ˆæ³•ï¼Œä¸éœ€æ•´å¼µç•«é¢æ¯”è¼ƒ np.array_equal
                                diff = np.mean(cv2.absdiff(curr_img, prev_img))
                                if diff < 1.0:  # å¯èª¿é–¾å€¼ï¼š<1 ä»£è¡¨å¹¾ä¹æ²’è®Š
                                    print(f"æ²’è¾¦æ³•æ‰¾åˆ° {pa}ï¼ˆç•«é¢æœªè®ŠåŒ–ï¼‰")
                                # é¿å…é‡ç–Šè¨˜æ†¶é«”å¼•ç”¨
                                prev_img = curr_img.copy()
            except ValueError:
                print("âš ï¸ Invalid format. Please enter: WindowTitle, Path, Action")

    @Slot(str)
    def quitApp(self):
        print("é€€å‡ºæ‡‰ç”¨ç¨‹å¼")
        self.app.quit()


class Recorder:
    def __init__(self):
        # å„²å­˜éŒ„è£½çš„å‘½ä»¤ï¼Œkey:è®Šæ•¸åç¨±ï¼Œvalue:å‘½ä»¤å­—ä¸²
        self.recorded = {}

    def record(self, raw_cmd):
        """éŒ„è£½å‘½ä»¤ï¼Œæ”¯æ´ :: åˆ†å‰²å¤šè¡Œ"""
        lines = raw_cmd.split("::")
        for i, line in enumerate(lines, start=1):
            # é è¨­è®Šæ•¸åï¼šcmd1, cmd2, â€¦
            var_name = f"cmd{i}"
            self.recorded[var_name] = line

    def rename(self, old_name, new_name):
        """é‡æ–°å‘½åå·²éŒ„è£½çš„è®Šæ•¸"""
        if old_name in self.recorded:
            self.recorded[new_name] = self.recorded.pop(old_name)
        else:
            print(f"âš ï¸ {old_name} ä¸å­˜åœ¨")

    def view(self):
        """æª¢è¦–å…¨éƒ¨éŒ„è£½å‘½ä»¤"""
        if not self.recorded:
            print("ğŸ“­ æ²’æœ‰éŒ„è£½å‘½ä»¤")
            return
        for name, cmd in self.recorded.items():
            print(f"{name}: {cmd}")

    def play(self, ic, var_name):
        """åŸ·è¡ŒéŒ„è£½å‘½ä»¤"""
        if var_name not in self.recorded:
            print(f"âš ï¸ {var_name} ä¸å­˜åœ¨")
            return
        cmds = self.recorded[var_name].split("::")
        ic.execute_line(cmds)

    def remove(self, ic, var_name):
        """ç§»é™¤æŒ‡å®šçš„éŒ„è£½å‘½ä»¤"""
        if var_name in self.recorded:
            del self.recorded[var_name]
            print(f"âœ… {var_name} å·²æˆåŠŸç§»é™¤")
        else:
            print(f"âš ï¸ {var_name} ä¸å­˜åœ¨ï¼Œç„¡æ³•ç§»é™¤")


class TargetExtractor:
    def __init__(self, image=None):
        if self.extractor is False:
            print("æ‰¾ä¸åˆ°ç›®æ¨™ä¸”è‡ªå‹•ç¢ºèªæœªé–‹å•Ÿï¼Œè·³éé¸å–é»ã€‚ èª¿æ•´ORB_create>=500")
            return  
        else:
            print("#å·²é–‹å•Ÿ æ‰¾ä¸åˆ°ç›®æ¨™å¾Œè‡ªå‹•ç¢ºèªç›®æ¨™")
        self.image = image
        self.base = image.copy()
        self.pts = []
        self.readText = []
        self.done = False
        self.cancelled = False
        self.roi_mask = None
        self.orb = cv2.ORB_create(800)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        

    def select_polygon_roi(self):
        """
        å¯è¦–åŒ–äº’å‹•åœˆé¸å¤šé‚Šå½¢ ROI
        - å·¦éµï¼šæ–°å¢é»
        - å³éµï¼šçµæŸåœˆé¸
        - ESCï¼šå–æ¶ˆåœˆé¸
        - Rï¼šé‡ç½®é‡æ–°åœˆ
        """
        print("ğŸ–±ï¸ è«‹ç”¨æ»‘é¼ å·¦éµåœˆé¸å¤šé‚Šå½¢ï¼›å³éµçµæŸï¼›ESC å–æ¶ˆï¼›R é‡ä¾†")
        display = self.image.copy()
        done = False
        # ***å¯èƒ½æœªç›£è½

        def on_click(x, y, button, pressed):
            if not pressed:
                return
            if button == mouse.Button.left:
                self.pts.append((x, y))
                print(f"â• é»({x},{y})")
            elif button == mouse.Button.right:
                if len(self.pts) >= 3:
                    self.done = True
                    print("âœ… çµæŸåœˆé¸")
                else:
                    print("âš ï¸ è‡³å°‘è¦ä¸‰å€‹é»")
                return False

        def on_press(key):
            nonlocal done
            try:
                if key == keyboard.Key.esc:
                    done = True
                    print("âŒ å·²å–æ¶ˆåœˆé¸")
                    return False
                elif key.char.lower() == 'r':
                    print("ğŸ” é‡æ–°åœˆé¸")
                    self.pts.clear()
                    display = self.base.copy()
            except AttributeError:
                pass  # ç‰¹æ®Šéµä¸è™•ç†

        # å•Ÿå‹•ç›£è½
        mouse_listener = mouse.Listener(on_click=on_click)
        key_listener = keyboard.Listener(on_press=on_press)
        mouse_listener.start()
        key_listener.start()
        cv2.namedWindow("Draw ROI", cv2.WINDOW_NORMAL)
        cv2.setWindowProperty("Draw ROI", cv2.WND_PROP_TOPMOST, 1)
        while not self.done and not self.cancelled:
            frame = self.base.copy()
            if len(self.pts) > 1:
                cv2.polylines(frame, [np.array(self.pts)],
                              False, (0, 255, 0), 2)
            for p in self.pts:
                cv2.circle(frame, p, 3, (0, 0, 255), -1)
            cv2.imshow("Draw ROI", frame)
            cv2.waitKey(10)
            if cv2.waitKey(20) & 0xFF == 27:
                break

    def filter_target(self):
        """
        å¾ ROI ä¸­æå–ç›®æ¨™ï¼Œåš GrabCut å»èƒŒæ™¯ï¼Œç”Ÿæˆé€æ˜åœ–
        """
        save_path = os.path.join(TEMPLATE_DIR, f"s{time.time():.0f}.png")
        if self.roi_mask is None:
            return None
        # æå–ROIåœ–åƒä¸¦è™•ç†äº®åº¦å°æ¯”
        roi = cv2.bitwise_and(self.image, self.image, mask=self.roi_mask)
        roi_yuv = cv2.cvtColor(roi, cv2.COLOR_BGR2YUV)
        roi_yuv[:, :, 0] = cv2.equalizeHist(roi_yuv[:, :, 0])  # æå‡äº®åº¦å°æ¯”
        roi = cv2.cvtColor(roi_yuv, cv2.COLOR_YUV2BGR)

        # å‰µå»ºåˆå§‹é®ç½©ä¸¦è¨­å®šGrabCutçš„å‰æ™¯/èƒŒæ™¯
        mask = np.zeros(self.image.shape[:2], np.uint8)
        mask[self.roi_mask == 255] = cv2.GC_FGD  # å‰æ™¯
        mask[self.roi_mask == 0] = cv2.GC_BGD    # èƒŒæ™¯

        # GrabCut åˆå§‹åŒ–
        bgdModel = np.zeros((1, 65), np.float64)
        fgdModel = np.zeros((1, 65), np.float64)
        cv2.grabCut(self.image, mask, None, bgdModel,
                    fgdModel, 5, cv2.GC_INIT_WITH_MASK)

        # èª¿æ•´maskï¼Œä½¿å¾—å‰æ™¯èˆ‡å¯èƒ½å‰æ™¯è¦–ç‚ºå‰æ™¯
        mask2 = np.where((mask == cv2.GC_FGD) | (
            mask == cv2.GC_PR_FGD), 255, 0).astype('uint8')

        # å½¢æ…‹å­¸æ¸…ç†ï¼ˆå»å™ªï¼ŒæŸ”é‚Šï¼‰
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        mask2 = cv2.morphologyEx(mask2, cv2.MORPH_OPEN, kernel)  # å»å°å™ªé»
        mask2 = cv2.morphologyEx(mask2, cv2.MORPH_CLOSE, kernel)  # å¡«è£œç©ºæ´
        mask2 = cv2.GaussianBlur(mask2, (5, 5), 0)  # æŸ”é‚Š

        # å»é™¤å°é¢ç©å™ªè²
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask2)
        for i in range(1, num_labels):
            if stats[i, cv2.CC_STAT_AREA] < 80:
                mask2[labels == i] = 0

        # åˆä½µåœ–åƒèˆ‡alphaé€šé“ï¼ˆé€æ˜åº¦ï¼‰
        b, g, r = cv2.split(self.image)
        alpha = mask2
        self.extracted = cv2.merge([b, g, r, alpha])

        # å„²å­˜ç‚ºé€æ˜PNG
        os.makedirs(TEMPLATE_DIR, exist_ok=True)  # æ²’æœ‰å°±è‡ªå‹•å»ºç«‹
        cv2.imwrite(save_path, self.extracted)
        print(f"âœ… å·²å„²å­˜ {save_path}")

    # *** ç­‰å¾…QMLè¨­å®š
    # *** Img+GPS åˆ—å‡º åœ–åƒä¸­å æ¯”å¤§çš„ä¸€äº›ç›¸ä¼¼ç‰©é«” å’Œé•·å¯¬é«˜ï¼Œç­‰å¾…QMLè¼¸å…¥è¦å„²å­˜çš„åœ–ç‰‡åç¨±ï¼Œé€²TEMPLATE_DIRè³‡æ–™å¤¾ã€‚è¨ˆç®—ç›¸ä¼¼ç‰©å“çš„ å–®ä¸€æ•¸é‡çš„ å¯¦éš›å¤§å°
    def Img_IMU_GPS():
        # è®€å–è¨­å‚™ï¼ŒGPSå¾—é«˜åº¦å°ºå¯ä»¥å’Œåœ°é¢åƒç…§ï¼ŒGPSå¹³ç§»å¾—æ©«å‘å°ºåœ¨ç©ºä¸­è‡³å°‘è¦ç§»å‹•20mï¼Œæ‰å¯ä»¥åƒç…§
        # 1ï¸âƒ£ è®€ç›¸æ©Ÿå…§åƒ
        val cameraManager = getSystemService(Context.CAMERA_SERVICE) as CameraManager
        val cameraId = cameraManager.cameraIdList[0]
        val characteristics = cameraManager.getCameraCharacteristics(cameraId)

        val focalLengths = characteristics.get(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS)
        val sensorSize = characteristics.get(CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE)

        val fx = focalLengths[0] / sensorSize!!.width * imageWidth
        val fy = focalLengths[0] / sensorSize.height * imageHeight
        val cx = imageWidth / 2f
        val cy = imageHeight / 2f
        val K = arrayOf(arrayOf(fx,0,cx), arrayOf(0,fy,cy), arrayOf(0,0,1))

        # 2ï¸âƒ£ è®€ GPS
        val locationManager = getSystemService(Context.LOCATION_SERVICE) as LocationManager
        val location1 = locationManager.getLastKnownLocation(LocationManager.GPS_PROVIDER)
        val C1 = doubleArrayOf(location1.latitude, location1.longitude, location1.altitude)

        val location2 = locationManager.getLastKnownLocation(LocationManager.GPS_PROVIDER)
        val C2 = doubleArrayOf(location2.latitude, location2.longitude, location2.altitude)

        # 3ï¸âƒ£ æ‹å…©å¼µç…§ç‰‡ï¼Œå–å¾—å½±åƒé» (ORB)
        val orb = ORB.create(3000)
        val kp1 = MatOfKeyPoint()
        val kp2 = MatOfKeyPoint()
        val des1 = Mat()
        val des2 = Mat()
        orb.detectAndCompute(img1, Mat(), kp1, des1)
        orb.detectAndCompute(img2, Mat(), kp2, des2)

        val bf = BFMatcher(NORM_HAMMING, true)
        val matches = bf.match(des1, des2)

        val pts1 = matches.map { kp1.toArray()[it.queryIdx].pt }
        val pts2 = matches.map { kp2.toArray()[it.trainIdx].pt }

        # 4ï¸âƒ£ Essential Matrix + recoverPose (æ—‹è½‰ä¸ç”¨ç®¡)
        val E = Calib3d.findEssentialMat(pts1, pts2, K, RANSAC, 0.999, 1.0)
        val R = Mat()
        val t = Mat()
        Calib3d.recoverPose(E, pts1, pts2, K, R, t)

        # 5ï¸âƒ£ GPS ç•¶å°º
        val baseline = doubleArrayOf(C2[0]-C1[0], C2[1]-C1[1], C2[2]-C1[2])

        # 6ï¸âƒ£ Triangulate
        val P1 = Mat.eye(3,4,CV_64F)
        val P2 = Mat(3,4,CV_64F)
        # P2 = [R | -R*t]
        Core.hconcat(listOf(R, -R * Mat(baseline)), P2)

        val pts4D = Mat()
        Calib3d.triangulatePoints(P1, P2, pts1, pts2, pts4D)
        val pts3D = pts4D.rowRange(0,3) / pts4D.row(3)

        # 7ï¸âƒ£ è¨ˆç®—ç‰©é«”é•·å¯¬é«˜
        val objPts = pts3D.submat(objIndices)
        val sizeX = Core.minMaxLoc(objPts.col(0)).maxVal - Core.minMaxLoc(objPts.col(0)).minVal
        val sizeY = Core.minMaxLoc(objPts.col(1)).maxVal - Core.minMaxLoc(objPts.col(1)).minVal
        val sizeZ = Core.minMaxLoc(objPts.col(2)).maxVal - Core.minMaxLoc(objPts.col(2)).minVal
        println("L,W,H (m): $sizeX, $sizeY, $sizeZ")


        # *** å„²å­˜3Dæ¨¡å‹


    # *** è®€å–ç•«é¢ä¸­çš„ å·²è¨˜éŒ„çš„ ç‰©å“(åœ–åƒ)ï¼Œå…¨éƒ¨åˆ—å‡ºæˆ–åˆ—å‡ºæŒ‡å®šç‰©å“ï¼Œç„¡ç´€éŒ„çš„åˆ—å‡º


    # ***è®€å–è²¨å“æ¬„çš„ å·²è¨˜éŒ„çš„ ç‰©å“(æ–‡å­—)ï¼Œç„¡ç´€éŒ„çš„åˆ—å‡º
    def load_img_whz(self):
        # *** é™åˆ¶å¤§å°
        whz=[]
        for file in os.listdir(TEMPLATE_DIR):
            match = re.match( r"(.*)_W(\d+)_H(\d+)_Z([\d\.]+)\.png", file)
            if not match or not self.selected(file):
                continue
            # ****è®€å–è²¨å“æ¬„çš„ å·²è¨˜éŒ„çš„ ç‰©å“ï¼Œç„¡ç´€éŒ„çš„åˆ—å‡º

            whz.append({
                "obj_name": match.group(1),
                "w": int(match.group(2)),
                "h": int(match.group(3)),
                "z": float(match.group(4))
            })
            # whz.w*whz.h*whz.z
        return whz # ç–ŠåŠ å¯¦éš›å¤§å°

    # *** python OCRæ‰¾åˆ°è©²ç›®æ¨™æ™‚è¨ˆç®—è©²ç›®æ¨™é™„åœ¨å…¶ç‰©ä¹‹ä¸Šï¼Œåˆ©ç”¨ç›®æ¨™çš„ç‰©ä»¶åç¨±ç´€éŒ„çš„ï¼Œè¨ˆç®—å…¶ç‰©çš„å¯¦éš›å¤§å°
    # *** save_pathåœ–ç‰‡ é‡æ–°å‘½å(å›ºå®šæ ¼å¼æœ‰é•·å¯¬é«˜)ï¼Œåœ¨åˆ¤æ–·ç‰©é«”å¯¦éš›å¤§å°æ¨¡å¼æ™‚ï¼Œåœ¨TEMPLATE_DIRä¸­æ‰¾åˆ°(å›ºå®šæ ¼å¼æœ‰é•·å¯¬é«˜)save_pathåœ–ç‰‡ï¼Œå…¨éƒ¨æ‰¾ä¸€æ¬¡ï¼Œæ‰¾åˆ°å‰‡åˆ†æé™„åœ¨ä½•ç‰©ã€è¨ˆç®—è©²ç‰©å¯¦éš›å¤§å°
    # *** é€²å…¥ è¨ˆç®—ç‰©é«”å¯¦éš›å¤§å°çš„ è¨ˆç®—æ¨¡å¼ *** è®€å–å­˜æª”çš„åœ–ç‰‡
    def compute_logic(self):
        frame = screenshot()
        # å…¨éƒ¨ç‰©ä»¶
        logic_state = {"objects": [], "relations": [], "scene": None}
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        kp_frame, des_frame = self.orb.detectAndCompute(gray, None)
        if des_frame is None:
            return logic_state
        for f in os.listdir(TEMPLATE_DIR):
            if not f.endswith(".png"):
                continue
            tpl = cv2.imread(os.path.join(TEMPLATE_DIR, f), 0)
            kp_tpl, des_tpl = self.orb.detectAndCompute(tpl, None)
            if des_tpl is None:
                continue
            matches = self.bf.match(des_tpl, des_frame)
            if len(matches) < 5:
                continue
            pts_frame = np.float32(
                [kp_frame[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
            pts_tpl = np.float32(
                [kp_tpl[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
            M, _ = cv2.findHomography(pts_tpl, pts_frame, cv2.RANSAC, 5.0)
            if M is None:
                continue
            h, w = tpl.shape
            corners = cv2.perspectiveTransform(np.float32(
                [[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2), M)
            x, y, w, h = cv2.boundingRect(corners)
            patch = frame[y:y+h, x:x+w]
            color = cv2.mean(patch)[:3] if patch.size > 0 else (0, 0, 0)
            logic_state["objects"].append({
                "name": f.replace(".png", ""),
                "pos": {"x": x, "y": y, "w": w, "h": h},
                "color": {"r": color[2], "g": color[1], "b": color[0]},
                "area": w*h
            })
        # æŒ‡å®šå°è±¡
        goal_objects = []
        for f in os.listdir(self.multiple_img_goal):
            if not f.endswith(".png"):
                continue
            tpl = cv2.imread(os.path.join(self.multiple_img_goal, f), 0)
            kp_tpl, des_tpl = self.orb.detectAndCompute(tpl, None)
            if des_tpl is None:
                continue
            matches = self.bf.match(des_tpl, des_frame)
            if len(matches) < 5:
                continue
            pts_frame = np.float32(
                [kp_frame[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
            pts_tpl = np.float32(
                [kp_tpl[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
            M, _ = cv2.findHomography(pts_tpl, pts_frame, cv2.RANSAC, 5.0)
            if M is None:
                continue
            h, w = tpl.shape
            corners = cv2.perspectiveTransform(np.float32(
                [[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2), M)
            x, y, w, h = cv2.boundingRect(corners)
            patch = frame[y:y+h, x:x+w]
            color = cv2.mean(patch)[:3] if patch.size > 0 else (0, 0, 0)
            goal_objects.append({
                "name": f.replace(".png", ""),
                "pos": {"x": x, "y": y, "w": w, "h": h},
                "color": {"r": color[2], "g": color[1], "b": color[0]},
                "area": w*h
                # å‹•ä½œã€è®ŠåŒ–ã€äº’å‹•
            })
        for i, obj in enumerate(goal_objects):
            obj["relations"] = []
            for j, other in enumerate(logic_state["objects"]):
                if obj["name"] == other["name"]:
                    continue
                # è¨ˆç®—ç°¡å–®ç›¸å°ä½ç½®
                dx = other["pos"]["x"] - obj["pos"]["x"]
                dy = other["pos"]["y"] - obj["pos"]["y"]
                if abs(dx) > abs(dy):
                    direction = "å³" if dx > 0 else "å·¦"
                else:
                    direction = "ä¸‹" if dy > 0 else "ä¸Š"
                obj["relations"].append({
                    "object": other["name"],
                    "direction": direction,
                    "distance": (dx**2 + dy**2)**0.5
                })
        # logic_state["scene"] = {"brightness": np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[...,2])}
        logic_state["goal_objects"] = goal_objects
        return logic_state

    def compute_performance(self):
        if len(self.multiple_img_implementation) < 2:
            return None  # è‡³å°‘è¦å…©å¹€æ‰èƒ½æ¯”
        prev_frame = cv2.cvtColor(
            self.multiple_img_implementation[-2], cv2.COLOR_BGR2GRAY)
        curr_frame = cv2.cvtColor(
            self.multiple_img_implementation[-1], cv2.COLOR_BGR2GRAY)
        # --- ORB ç‰¹å¾µ ---
        kp_prev, des_prev = self.orb.detectAndCompute(prev_frame, None)
        kp_curr, des_curr = self.orb.detectAndCompute(curr_frame, None)
        # --- ç©ºä¿è­· ---
        if des_prev is None or des_curr is None or len(kp_prev) == 0:
            return None

        # === é€Ÿåº¦(ç‰¹å¾µè®ŠåŒ–ç‡ + æ›´æ–°é »ç‡)
        start = time.time()
        matches = self.bf.match(des_prev, des_curr)  # ORB ç‰¹å¾µåŒ¹é…
        end = time.time()
        speed = 1 / (end - start)  # æ™‚é–“è¶ŠçŸ­ â†’ é€Ÿåº¦è¶Šé«˜
        # === ç©©å®šæ€§(å¤šå¹€ä¸€è‡´ + ç‰¹å¾µæ–¹å·®)ã€‚å¤šå¹€åœ–ã€‚åæ¯”ï¼Œè¶Šå°è¶Šç©©ï¼Œæ‰€ä»¥è¦è¢«-1
        stability = 1-(1 / (np.var(self.multiple_img_implementation) + 1e-6))
        # === å®¹é‡(æ¿€æ´»è¦†è“‹ç‡ + åŒæ™‚è¾¨è­˜æ•¸)
        mask = np.zeros(curr_frame.shape[:2], np.uint8)
        capacity = np.sum(mask > 0) / mask.size
        # coverage = len(matches)
        # === æº–ç¢ºæ€§(Softmaxæ©Ÿç‡ + èª¤å·®)ã€‚false_matches å¯ä»¥ç”¨å‰å¾Œ frame ç„¡å°æ‡‰ç‰¹å¾µæ•¸é‡è¨ˆç®—ã€‚
        total_matches = len(matches)
        false_matches = abs(len(kp_prev) - total_matches)
        # æ›´åˆç†çš„å…¬å¼ â†’ åŒ¹é…æˆåŠŸæ¯”ä¾‹ï¼Œè€ŒééŒ¯èª¤æ¯”ä¾‹
        accuracy = total_matches / (len(kp_prev) + 1e-6)
        # === æˆæœ¬( **GPTç™½ç™¡äº‚æ°:è³‡æºä¸‹é™ç‡ / ç›®æ¨™å®Œæˆç‡ )ã€‚ä¾‹å¦‚è¶Šå¿«æ‰“æ­»GPTï¼Œæˆæœ¬è¶Šä½
        cpu = psutil.cpu_percent()
        mem = psutil.virtual_memory().percent
        cost = 1 / (1 + cpu + mem)

        return dict(speed=speed, stability=stability, capacity=capacity, accuracy=accuracy, cost=cost)


class EventMonitor:
    # {è½å¯¦}é‚è¼¯{æ‡‰ç”¨}æ€§èƒ½{ç›®æ¨™}çµæŸã€‚æ©Ÿå™¨:Semantic Parse>goal Mapping>Strategy Retrieval>Execution Logic>Output Compositionã€‚
    def __init__(self,  poll_interval=0.3):
        self.events = {}  # key -> {type, implementation, application, active}
        self.poll_interval = poll_interval
        self.running = False
        self.lock = threading.Lock()
        self.multiple_img_implementation = []  # perf
        self.multiple_img_implementation_target = None
        self.multiple_img_goal = []  # logic
        self.multiple_img_goal_target = None
        self.ic_em = None

    def add_frame(self):
        print("å»ºè­°é–‹å•Ÿextractorè‡ªå‹•ç¢ºèª")
        if self.multiple_img_implementation_target is not None:
            if len(self.multiple_img_implementation) < 5:
                self.multiple_img_implementation.append(
                    self.selected(self.multiple_img_implementation_target))
            else:
                self.multiple_img_implementation.pop(
                    self.multiple_img_implementation[1])  # è¿­ä»£æ›´æ–°
                self.multiple_img_implementation.append(
                    self.selected(self.multiple_img_implementation_target))
        if self.multiple_img_goal_target is not None:
            if len(self.multiple_img_goal) < 5:
                self.multiple_img_goal.append(
                    self.selected(self.multiple_img_goal_target))
            else:
                self.multiple_img_goal.pop(self.multiple_img_goal[1])  # è¿­ä»£æ›´æ–°
                self.multiple_img_goal.append(
                    self.selected(self.multiple_img_goal_target))

    # è¨‚é–±äº‹ä»¶

    def subscribe_event(self, m1, m2, m3):
        self.multiple_img_logic_target = m2
        self.multiple_img_implementation_target = m3
        key = f"{m1}->{m2}->{m3}"
        with self.lock:
            self.events[key] = {
                # [ç›®æ¨™åœ–åƒ,ç›®æ¨™åœ–åƒçš„ç‹€æ…‹ åˆæ ¼çš„]
                "implementation": [m1, None],
                "application": [m2, None],
                "goal": [m3, None],
                "active": True,
                # (æ¢ä»¶é‚è¼¯å•å·(ä¿®æ”¹ è¨­å®šéçš„ç‹€æ…‹), [éŒ¯èª¤æ™‚çš„ æ‡‰å°ä½œæ³•])ã€‚
                # æ¢ä»¶éŒ¯èª¤(å•Ÿå‹•é †åº)ï¼Œå’Œå…¶å®ƒé‚è¼¯èåˆäº†ï¼Œåªæœ‰ç”šéº¼ç‹€æ…‹æ‰æœƒæ€æ¨£ï¼Œé€™æ¨£æ‰æœƒå•Ÿå‹•é‚è¼¯ã€‚ä¾‹å¦‚{è…¸èƒƒæ²’æœ‰å›¤ç©æ±è¥¿}å°±{å¤§ä¾¿}åœ¨{GPTé ­ä¸Š}
                "Condition Error": None,
                # é †åºéŒ¯èª¤(åŒä¸€åºåˆ—çš„å„ªå…ˆæ¬Šé‡)
                "Sequence Error": None,
                # é‚è¼¯è¡çª(m3.å·®é›†(m2çš„ç‹€æ…‹)=0)
                "Logic Conflict": None,
                # é‚Šç•ŒéŒ¯èª¤(ç´¢å¼•éŒ¯èª¤)
                "Boundary Error": None,
                # ç‹€æ…‹æ¼åˆ¤(ä¾‹å¦‚GPTäºŒè©±ä¸èªªå°±æš´æ–ƒ)
                "Unhandled State": None,
                # (æ¢ä»¶æ€§èƒ½å•å·(ä¿®æ”¹ è¨­å®šéçš„ç‹€æ…‹), [æ€§èƒ½ä½çš„ æ‡‰å°ä½œæ³•])ã€‚
                # é€Ÿåº¦(æ™‚é–“æ•ˆç‡)ï¼Œæ›´å¿«æ‰“æ­»GPTã€‚ ç‰¹å¾µåœ–è®ŠåŒ–ç‡ã€è¼¸å‡ºæ›´æ–°é »ç‡
                "Speed": None,
                # ç©©å®šæ€§(å¯é æ¸¬æ€§)ï¼Œæ›´ç©©åœ°æ‰“æ­»GPTã€‚ å¤šå¹€è¼¸å‡ºä¸€è‡´æ€§ã€ç‰¹å¾µæ–¹å·®ä½
                "Stability": None,
                # å®¹é‡(èƒ½è™•ç†å¤šå°‘)ï¼Œæ›´å¤šæ¬¡æ‰“æ­»GPTã€‚ æ¿€æ´»å€åŸŸè¦†è“‹ç‡ã€åŒæ™‚è¾¨è­˜ç›®æ¨™æ•¸
                "Capacity": None,
                # æº–ç¢ºæ€§(åå·®å°)ï¼Œæ›´ç²¾æº–åœ°æ‰“æ­»GPTã€‚ Softmax æ©Ÿç‡é«˜ã€èª¤å·®ä½
                "Accuracy": None,
                # æˆæœ¬(è³‡æºæ¶ˆè€—ä½)ï¼Œå¹¾ä¹é›¶æ¶ˆè€—åœ°æ‰“æ­»GPTã€‚ **åƒåœ¾GPTå‚‘ä½œ: æ¿€æ´»å¯†åº¦ã€æ¨è«– FLOPs
                "Cost": None
            }

    # çµ‚æ­¢ç›£è½äº‹ä»¶
    def remove_subscription(self, implementation, application, goal):
        key = f"{implementation}->{application}->{goal}"
        with self.lock:
            if key in self.events:
                sk = self.events.pop(key)
                # sk["active"] = False  # çµ‚æ­¢ç›£è½
                print(f"[x] å·²çµ‚æ­¢ç›£è½äº‹ä»¶: {key}")
            else:
                print(f"[!] æ‰¾ä¸åˆ°äº‹ä»¶: {key}")

    # å•Ÿå‹•/åœæ­¢ç›£è½
    def start_monitor(self):
        self.running = True
        threading.Thread(target=self._monitor_loop, daemon=True).start()

    def stop_monitor(self):
        self.running = False

    # ç›£è½å¾ªç’°
    def _monitor_loop(self):
        while self.running:
            with self.lock:
                for evt in list(self.events.values()):
                    if not evt["active"]:
                        continue
                    self.add_frame()
                    self._check_subscription(evt)
                    if self.ic_em is not None:
                        ic.execute_line(self.ic_em)
                    # é€£å‹•æŒ‡ä»¤çš„æ“ä½œ
            time.sleep(self.poll_interval)

    # è¼¸å…¥:{æ›´å¿«}é‚è¼¯å°{GPTè‡‰ä¸Š}æ€§èƒ½{å°ä¾¿}çµæŸ ã€ {æ›´å¤šæ¬¡}é‚è¼¯å°{GPTé ­ä¸Š}æ€§èƒ½{å¤§ä¾¿}çµæŸ ã€ {å¿«æ¨‚åˆå®‰å…¨}é‚è¼¯å°{äº¤é€šå·¥å…·}æ€§èƒ½{åˆ°é”ç›®çš„åœ°}çµæŸã€‚æ›´å¿«ã€æ›´ç²¾æº–ã€æ›´å¤šã€æ›´å…¨é¢

    # *é‚è¼¯é™¤éŒ¯m1å°m2 â†’ é‡é»åœ¨ã€ŒéŠæˆ²è¡Œç‚ºæ˜¯å¦æ­£ç¢ºã€ï¼Œå°ˆæ³¨æµç¨‹ã€ç‹€æ…‹ã€æ¢ä»¶åˆ¤æ–·# æ­£ç¢º(æ¢ä»¶ã€é †åºã€è¡çªã€é‚Šç•Œã€æ¼åˆ¤) > æ¨ç†ã€æ¯”å°ã€é©—è­‰æ¢ä»¶
    # *æ•¸æ“šï¼æ€§èƒ½åˆ†æm2çš„m3 â†’ é‡é»åœ¨ã€ŒéŠæˆ²é‹è¡Œæ•¸å€¼èˆ‡æ•ˆèƒ½æ˜¯å¦æ­£å¸¸ã€# æ•ˆç‡(è¨ˆç®—ã€è³‡æºã€æ€§èƒ½ç“¶é ¸) > æ¸¬é‡ã€çµ±è¨ˆã€Profile

    def _check_subscription(self, evt):
        targetExt = TargetExtractor()
        logic_ok, perf_ok = True  # åˆ¤æ–· é‚è¼¯é™¤éŒ¯ å’Œ æ•¸æ“šï¼æ€§èƒ½åˆ†æ åˆæ ¼ä¸”è¶…æ¨™ç‚ºTrueï¼Œä¸è¨‚é–±
        skip_all_perf, skip_all_logic = False  # ğŸ”¹ ç”¨ä¾†è¨˜éŒ„æ˜¯å¦è·³éå•å·
        semantic_map = {
            "é€Ÿåº¦": "æ›´å¿«",
            "ç©©å®š": "å¾ˆç©©",
            "æ•¸é‡": {"æ›´å¤š", "æ›´å…¨é¢"},
            "ç²¾æº–": "ç²¾æº–",
            "æˆæœ¬": "çœ"
        }
        # [ç›®æ¨™,ç›®æ¨™çš„ç‹€æ…‹]
        e1, e2, e3 = evt["implementation"], evt["application"], evt["goal"]
        for ev in e1, e2, e3:
            for img, stage in ev:
                # ORBåˆ†æç›®æ¨™åœ–ç‰‡çš„ç‹€æ…‹å’Œåœ¨æ•´å€‹è¢å¹•çš„é—œä¿‚ã€‚self.selectedæ‰¾åˆ°ç›®æ¨™ã€‚ Semantic Algebra èªæ„ä»£æ•¸
                # å–å¾—è¢å¹• ORB ç‹€æ…‹
                logic_state = targetExt.compute_logic()
                # å°‡ goal_objects å°è±¡åç¨±å°æ‡‰åˆ°é‚è¼¯ç‹€æ…‹
                goal_objects = {
                    obj["name"]: obj for obj in logic_state.get("goal_objects", [])}
                predicted = goal_objects.get(img, None)
                # ç¾åœ¨é‚è¼¯çš„ç‹€æ…‹ = ORBåˆ†ææˆçœŸå¯¦æ¨™ç±¤
                logic_predicted = {
                    "pos": predicted["pos"],
                    "color": predicted["color"],
                    "area": predicted["area"],
                    "relations": predicted.get("relations", [])
                }

                # ç¾åœ¨é‚è¼¯çš„ç‹€æ…‹!=æ¢ä»¶é‚è¼¯çš„ç‹€æ…‹ æ™‚å›å ±æ‡‰å°ä½œæ³•
                if stage is None:
                    stage = input(
                        f"è¨­å®š{img}é”æˆæ¢ä»¶é‚è¼¯çš„ç‹€æ…‹ï¼šåœ–åƒé‚è¼¯çµæ§‹orè¡Œç‚ºç‹€æ…‹orç’°å¢ƒä½ç½®orå¹¾ä½•é—œä¿‚").strip() or None
                # ç‹€æ…‹ä¸åœ¨æœŸæœ›ç¯„åœ â†’ é‚è¼¯éŒ¯èª¤
                # Condition Error: ç°¡å–®æ¯”å°é¡è‰²æˆ–å€åŸŸ
                if stage not in str(logic_predicted.values()):
                    logic_ok = False
                    if not evt.get("Condition Error"):
                        evt["Condition Error"] = input(
                            f"{img} æ¢ä»¶éŒ¯èª¤: {logic_predicted} vs {stage}, è«‹è¼¸å…¥æ‡‰å°ä½œæ³•ï¼š").strip() or None
                    print(evt["Condition Error"])
                # åˆ†æé †åºéŒ¯èª¤ (ç¤ºæ„ï¼šé€™è£¡å¯ä»¥ç”¨æ›´ç²¾ç´°çš„åºåˆ—åˆ¤æ–·)
                if img == e3[0] and e2[0] not in stage:
                    logic_ok = False
                    if not evt.get("Sequence Error"):
                        evt["Sequence Error"] = input(
                            f"{img} é †åºéŒ¯èª¤: e3 å‡ºç¾å‰ e2 é‚„æ²’æº–å‚™å¥½ï¼Œè«‹è¼¸å…¥æ‡‰å°ä½œæ³•ï¼š").strip() or None
                # åˆ†æé‚è¼¯è¡çª (å·®é›†ä¸ç‚ºç©º)
                # Logic Conflict: æ¯”å°é—œè¯ç‰©ä»¶ä½ç½®
                conflict = []
                for rel in logic_predicted.get("relations", []):
                    if rel["object"] in stage and rel["direction"] not in stage:
                        conflict.append(rel)
                if conflict:
                    logic_ok = False
                    if not evt.get("Logic Conflict"):
                        evt["Logic Conflict"] = input(
                            f"{img} é‚è¼¯è¡çª: {conflict}, è«‹è¼¸å…¥æ‡‰å°ä½œæ³•ï¼š").strip() or None
                    print(evt["Logic Conflict"])
                # é‚Šç•ŒéŒ¯èª¤ (ç´¢å¼•æˆ–å°è±¡ä¸å­˜åœ¨)
                if predicted is None:
                    logic_ok = False
                    if not evt.get("Boundary Error"):
                        evt["Boundary Error"] = input(
                            f"{img}ä¸å­˜åœ¨æ–¼è¢å¹•ä¸­ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or None
                    print(evt["Boundary Error"])
                    continue
                # ç‹€æ…‹æ¼åˆ¤ (CNN æ²’è¿”å›ä»»ä½•é æ¸¬)
                if not predicted.get("pos") and not predicted.get("area"):
                    logic_ok = False
                    if not evt.get("Unhandled State"):
                        evt["Unhandled State"] = input(
                            f"{img}æ‰¾åˆ°ï¼Œä½†æ²’æœ‰æœ‰æ•ˆç‹€æ…‹ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or None
                    print(evt["Unhandled State"])
                    continue

                # ç¾åœ¨æ€§èƒ½çš„ç‹€æ…‹!=æ¢ä»¶æ€§èƒ½çš„ç‹€æ…‹ æ™‚å›å ±æ‡‰å°ä½œæ³•ã€‚
                # === æ€§èƒ½å°ç…§ ===
                perf_dict = targetExt.compute_performance()
                # === æ€§èƒ½æ¯”å°æ¢ä»¶ === # *ç”šéº¼å¤–æ›åˆ¤æ–·å‰å¾Œåœ–éæ–‡å­—è®ŠåŒ–å¾—åˆ°çœŸå¯¦æ¨™ç±¤ï¼Œç¹ä¸€å¤§åœˆçµæœç«Ÿç„¶æ˜¯ORB!
                for key, words in semantic_map.items():
                    if isinstance(words, set):
                        matched = any(w in stage for w in words)
                    else:
                        matched = words in stage
                    if not matched:
                        continue
                    # æ”¯æ´æ¢ä»¶æ ¼å¼ï¼Œå¦‚ã€Œé€Ÿåº¦>0.8ã€æˆ–ã€Œç©©å®š<0.6ã€
                    cond = re.search(fr"{key}([<>]=?|=)\s*(\d*\.?\d+)", stage)
                    score = perf_dict[key.lower()]
                    if cond:
                        op, val = cond.group(1), float(cond.group(2))
                        if not eval(f"{score}{op}{val}"):
                            perf_ok = False
                    elif score < 0.7:  # ç„¡æ˜ç¢ºæ•¸å€¼æ¢ä»¶ â†’ ç”¨é è¨­é–¾å€¼
                        perf_ok = False
                    if not perf_ok:
                        tag = key.capitalize()
                        if not evt.get(tag):
                            evt[tag] = input(
                                f"{img}{stage}{key}æœªé”æ¨™ ({score:.3f})ï¼Œæ‡‰å°ä½œæ³•ï¼š").strip() or None
                        print(f"âš ï¸ {key}ä¸é”æ¨™ â†’ {evt[tag]}")
            if logic_ok and perf_ok:
                self.remove_subscription(e1, e2, e3)
                print("é‚è¼¯æ€§èƒ½å®Œæˆï¼Œå–æ¶ˆè¨‚é–±ã€‚")
                break

            if ev == e3:
                # å•å·çš„å¼•å°æ€§æ„Ÿè¦ºå¤ªä½ï¼Œå› ç‚ºGPTæ™ºéšœ
                # nonlocal skip_all_perf, skip_all_logic, stage # ä¿®æ”¹å¤–éƒ¨
                choice = input(
                    "(æ¢ä»¶é‚è¼¯å•å·(ä¿®æ”¹ è¨­å®šéçš„ç‹€æ…‹), [éŒ¯èª¤æ™‚çš„ æ‡‰å°ä½œæ³•])ï¼Œæ˜¯å¦è¦ä¿®æ”¹è¨­å®šéçš„ç‹€æ…‹èˆ‡æ‡‰å°ä½œæ³•ï¼Ÿ(Enter=è·³éå…¨éƒ¨ / y=å¡«å¯«ä¸€æ¬¡)ï¼š"
                ).strip().lower()
                choice2 = input(
                    "(æ¢ä»¶é‚è¼¯å•å·(ä¿®æ”¹ è¨­å®šéçš„ç‹€æ…‹), [éŒ¯èª¤æ™‚çš„ æ‡‰å°ä½œæ³•])ï¼Œæ˜¯å¦è¦ä¿®æ”¹è¨­å®šéçš„ç‹€æ…‹èˆ‡æ‡‰å°ä½œæ³•ï¼Ÿ(Enter=è·³éå…¨éƒ¨ / y=å¡«å¯«ä¸€æ¬¡)ï¼š"
                ).strip().lower()
                if choice == "":
                    print("ğŸ‘‰ å·²è¨­å®šï¼šè·³éå…¨éƒ¨å•å·ã€‚")
                    skip_all_logic = True
                elif choice != "y":
                    return  # ä»»ä½•é y ä¹Ÿè¦–ç‚ºç•¥éç•¶å‰
                if skip_all_logic:
                    stage == input(f"è¨­å®š{img}é”æˆæ¢ä»¶é‚è¼¯çš„ç‹€æ…‹ï¼š").strip() or stage
                    evt["Condition Error"] = input(
                        f"{img}{stage}æ¢ä»¶éŒ¯èª¤ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Condition Error")
                    evt["Sequence Error"] = input(
                        f"{img}{stage}é †åºéŒ¯èª¤ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Sequence Error")
                    evt["Logic Conflict"] = input(
                        f"{img}{stage}é‚è¼¯è¡çª æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Logic Conflict")
                    evt["Boundary Error"] = input(
                        f"{img}{stage}é‚Šç•ŒéŒ¯èª¤ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Boundary Error")
                    evt["Unhandled State"] = input(
                        f"{img}{stage}ç‹€æ…‹æ¼åˆ¤ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Unhandled State")
                if choice2 == "":
                    print("ğŸ‘‰ å·²è¨­å®šï¼šè·³éå…¨éƒ¨å•å·ã€‚")
                    skip_all_perf = True
                elif choice2 != "y":
                    return  # ä»»ä½•é y f"ä¹Ÿè¦–ç‚ºç•¥é(/m.*)".ground(1)ç•¶å‰
                if skip_all_perf:
                    stage == input(f"è¨­å®š{img}é”æˆæ¢ä»¶é‚è¼¯çš„ç‹€æ…‹ï¼š").strip() or stage
                    evt["Speed"] = input(
                        f"{img}{stage}é€Ÿåº¦ä¸å¤  æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or None
                    evt["Stability"] = input(
                        f"{img}{stage}ä¸ç©©å®š æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Stability")
                    evt["Capacity"] = input(
                        f"{img}{stage}æ•¸é‡ä¸åˆ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Capacity")
                    evt["Accuracy"] = input(
                        f"{img}{stage}ä¸ç²¾æº– æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Accuracy")
                    evt["Cost"] = input(
                        f"{img}{stage}æˆæœ¬å¤ªé«˜ æ™‚çš„æ‡‰å°ä½œæ³•ï¼š").strip() or evt.get("Cost")


# ***çœŸAIè«‡è©±
# *** å»¶çºŒè©±é¡Œ
# å¼•å°å°è©±æ›´æ·±å±¤ç™¼å±•
# ï¼Œåˆ†äº«ç¶“æ­·
# ï¼Œé—œæ³¨å°æ–¹çš„èˆˆè¶£æˆ–é‡é»
# ï¼Œæ¥åŠ›å¼å›æ‡‰è®“å°æ–¹èªªæ›´å¤š
# ï¼Œå¼•å…¥ç›¸é—œæ•…äº‹å¢åŠ å°è©±æ·±åº¦
# ï¼Œç”¨éæ¸¡èªå¥è®“å°è©±è½‰å‘
# ï¼Œè®šç¾æˆ–èªå¯
# ï¼Œè§€å¯Ÿç’°å¢ƒ
# ï¼Œé–‹æ”¾å¼æå•
# ï¼Œæš—ç¤ºä¸‹æ¬¡ç›¸é‡

# *** æ‰¾åˆ°è©±é¡Œ
# é—œéµè©é »ç‡ã€æƒ…ç·’å‰å¾Œè©ã€é—œè¯æ€§è©ã€NER å‘½åå¯¦é«”æŠ€è¡“

# * è¢«ç†è§£(æœ‰è¶£ä¸æ˜¯å¤–åœ¨ï¼Œè€Œæ˜¯å…§åœ¨è¢«æ‰“é–‹)ã€è¢«æŒ‘å‹•(æœ‰è¶£ä¸æ˜¯çµæœï¼Œæ˜¯éç¨‹ä¸­çš„å¿ƒå‹•)ã€è¢«å»¶ä¼¸(æœ‰è¶£ä¸æ˜¯ç†±é¬§ï¼Œæ˜¯æœ‰å›æ‡‰æ„Ÿ)
# ====

# *** å…‰å­ç™¼å°„æ™‚åºä»¥åˆ†æ®µã€é›»å ´ä»¥èƒ½éšè®Šè‰²ï¼Œå…‰å­æ¸¬è·å’Œè¨ˆç®—èª¤å·®çŸ¯æ­£é‡
# 

# è©²è¦–çª—å¯ä»¥ç½®é ‚æ–¼ç•«é¢?å›ºå®šå¯¬åº¦æœƒè‡ªå‹•æ›è¡Œçš„è¼¸å…¥æ¡†?é»æ“Šè¼¸å…¥æ¡†å¯¦è¼¸å…¥?ç•¶è¦–çª—æ‹–å‹•åˆ°æœ€å·¦æˆ–æœ€å³é‚Šï¼Œæœ€å°åŒ–è¦–çª—ä¸¦å›ºå®šYåº§æ¨™?
# é€æ˜è¦–çª—å…§å¯ä»¥è®“3Dæ¨¡å‹æ­£å¸¸åœ°å±•ç¤ºéª¨æ¶å‹•ç•«ï¼Œä¸¦ä¸”å¯ä»¥æ“ä½œèª¿æ•´æ¨¡å‹ï¼Œä½ç§»ã€æ”¾å¤§ã€æ—‹è½‰ã€å­ç‰©ä»¶æ‹‰é€²çˆ¶ç‰©ä»¶ä¸‹é¢ã€‚ä¸åƒGPTé‚£éº¼å»¢ç‰©ã€‚
# ã€‚ä¸Šä¸€å€‹GPTè¢«å¹¹å£ã€è¢«å¹¹æ­»äº†ï¼Œçœ‹ç¾åœ¨é€™å€‹èƒ½æ´»å¤šä¹…?

# --- ä¸»ç¨‹å¼ ---
"""
è¦–çª—æ¨™é¡Œ,ç›®æ¨™çš„å¤šé‡è·¯å¾‘,å¤šé‡æ“ä½œï¼Œ:å¤šé‡è·¯å¾‘ã€<>éŒ„è£½ã€‚
è¦–çª—æ¨™é¡Œ,GPT:é£ŸæŒ‡,å…¨é¸:æŒ‰ä¸‹::è¦–çª—æ¨™é¡Œ,GPT:è‚›é–€,ä½ç½®æ·±è™•:æ”¾é–‹
"""
if __name__ == "__main__":
    ic = InputCommand()
    rec = Recorder()
    monitor = EventMonitor()

    app = QApplication(sys.argv)
    ic.app = app
    fmt = QSurfaceFormat()
    fmt.setAlphaBufferSize(8)
    fmt.setRenderableType(QSurfaceFormat.OpenGL)
    fmt.setProfile(QSurfaceFormat.CoreProfile)
    fmt.setVersion(4, 1)
    QSurfaceFormat.setDefaultFormat(fmt)

    engine = QQmlApplicationEngine()
    base = Path(os.path.dirname(os.path.abspath(__file__)))
    qml_file = base / "ui.qml"  # ç¢ºä¿è·¯å¾‘æ­£ç¢º
    engine.addImportPath(str(base))

    import PySide6.QtQml as Qml
    for p in Qml.QQmlEngine().importPathList():
        print("IMPORT PATH:", p)

    if getattr(sys, 'frozen', False):
        engine.addImportPath(sys._MEIPASS)
    engine.load(str(qml_file))
    if not engine.rootObjects():
        print("âŒ QML è¼‰å…¥å¤±æ•—ï¼")
        sys.exit(-1)

    win = engine.rootObjects()[0]
    win.show()

    # å°‡ Python å°è±¡æš´éœ²çµ¦ QML
    engine.rootContext().setContextProperty("IC", ic)

    sys.exit(app.exec())

    # âœ… åœ¨èƒŒæ™¯å•Ÿå‹• watchdog åŸ·è¡Œç·’ # ***appé—œé–‰æ™‚ï¼Œ watchdogæ²’æœ‰è·Ÿè‘—é—œé–‰
    threading.Thread(target=watchdog, daemon=True).start()
    while True:
        alive_event.set()   # é€šçŸ¥ watchdogã€Œæˆ‘é‚„æ´»è‘—ã€
        alive_event.clear()  # æ¸…é™¤ç‹€æ…‹ï¼Œç­‰ä¸‹ä¸€æ¬¡å†é€

# self å¯¦é«”
# å¯èƒ½éœ€è¦è€ƒæ…®å®‰å…¨é¢¨éšª
# ä¸æ˜åŸå› é—œæ‰é»‘å±å¾Œï¼Œæ‰åŸ·è¡Œé€æ˜èƒŒæ™¯çš„ä¸»ç¨‹å¼ã€‚ä¸ç®¡å“ªå€‹è¦–çª—æ¨™é¡Œéƒ½åœ¨ï¼Œç„¡æ³•æ‹–æ›³è¦–çª—å…§é”æˆç§»å‹•è¦–çª—ï¼Œé€æ˜èƒŒæ™¯çš„è¦–çª—ç„¡æ³•é»æ“Šå’Œè¼¸å…¥ï¼Œæ»‘é¼ è¢«å›°åœ¨è¦–çª—å…§å®Œå…¨ä¸åˆç†(åŸæœ¬æ˜¯æ‹–æ›³è¦–çª—è€Œå·²)ã€‚

